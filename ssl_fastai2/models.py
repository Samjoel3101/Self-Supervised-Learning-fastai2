# AUTOGENERATED! DO NOT EDIT! File to edit: 03_models.ipynb (unless otherwise specified).

__all__ = ['CGDResNet', 'arch_map', 'rep_resnet', 'L2Norm', 'lin_layer', 'BatchDropBlock', 'RepresentationHead',
           'create_bottleneck', 'SSLModel', 'ImageRepEncoder', 'SIMClrHead', 'cgd_encoder', 'available_architectures',
           'SIMClrModel', 'SPoC', 'GeM', 'cgd_descriptors', 'cgd_config']

# Cell
from .imports import *
from .utils import Encoding

# Cell
from torchvision.models.resnet import ResNet
from torchvision.models import resnet as resnet_module
from torchvision.models.utils import load_state_dict_from_url

# Cell
arch_map = {'resnet18': {'block': resnet_module.BasicBlock, 'layers': [2, 2, 2, 2]},
            'resnet34': {'block': resnet_module.BasicBlock, 'layers': [3, 4, 6, 3]},
            'resnet50': {'block': resnet_module.Bottleneck, 'layers': [3, 4, 6, 3]},
            'resnet101': {'block': resnet_module.Bottleneck, 'layers': [3, 4, 23, 3]},
            'resnet152': {'block': resnet_module.Bottleneck, 'layers': [3, 8, 36, 3]}}

class CGDResNet(ResNet):
  def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,
                 groups=1, width_per_group=64, replace_stride_with_dilation=None,
                 norm_layer=None):
    super().__init__(block, layers, num_classes, zero_init_residual, groups, width_per_group, replace_stride_with_dilation, norm_layer)
    self.inplanes = 128*block.expansion
    self.layer3 = self._make_layer(block, 256, layers[2], stride=1,
                                   dilate = False)

def _rep_resnet(arch, block, layers, pretrained, progress, **kwargs):
    model = CGDResNet(block, layers, **kwargs)
    if pretrained:
        state_dict = load_state_dict_from_url(resnet_module.model_urls[arch],
                                              progress=progress)
        model.load_state_dict(state_dict, strict = False)
    return model

# Cell
def rep_resnet(arch, pretrained, progress = True, **kwargs):
  global arch_map
  return _rep_resnet(arch, arch_map[arch]['block'], arch_map[arch]['layers'], pretrained = pretrained, progress = progress, **kwargs)

# Cell
class L2Norm(nn.Module):
  def forward(self, x):
    assert x.dim() == 2 , "Input Size should be [batch_size, represenation_dim]"
    return F.normalize(x, p = 2, dim = -1)

def lin_layer(in_feat, out_feat, bn = False, act_cls = nn.ReLU):
  linear = [nn.Linear(in_feat, out_feat)]
  if bn:
    linear += [nn.BatchNorm1d(out_feat)]
    linear += [act_cls()]
  return nn.Sequential(*linear)

class BatchDropBlock(nn.Module):
  def __init__(self, h_ratio, w_ratio):
    super().__init__()
    self.h_ratio = h_ratio
    self.w_ratio = w_ratio

  def forward(self, x):
    if self.training:
      h, w = x.size()[-2:]
      hr   = round(self.h_ratio*h )
      wr   = round(self.w_ratio*w )
      sx   = random.randint(0, h-hr)
      sy   = random.randint(0, w-wr)
      mask = x.new_ones(x.size())
      mask[:, :, sx:sx+hr, sy:sy+wr] = 0
      x = x*mask
    return x

class RepresentationHead(nn.Module):
  def __init__(self, in_feat, rep_dim,
               block_ratio = (0.3, 1.), bdb = True,
               pool = nn.AdaptiveAvgPool2d(1),
               lin_kwargs = {'bn':False, 'act_cls':nn.ReLU}, conv_kwargs = {'act_cls':nn.ReLU},
               use_conv_reduction = True): # Use ks 1 Conv2d to reduce from 2048 to 1024 dim if False use a lin_layer

    super().__init__()
    self.use_bdb = bdb
    self.bdb = BatchDropBlock(*block_ratio)

    pool_layer = pool

    if use_conv_reduction:
      conv_reduction = ConvLayer(2048, 1024, ks = 1, **conv_kwargs)
      lin_head = lin_layer(1024, rep_dim, **lin_kwargs)
      head = [conv_reduction, Flatten(), lin_head]
    else:
      head = [Flatten(), lin_layer(2048, rep_dim, **lin_kwargs)]

    regularisation = L2Norm()
    layers = [pool_layer, *head, regularisation]

    self.layers = nn.Sequential(*layers)

  def forward(self, x):
    if self.use_bdb:
      x = self.bdb(x)
    return self.layers(x)

# Cell
def create_bottleneck(planes, use_bottleneck):
  try:
    grps = 2048//(planes*(1024//planes))
  except ZeroDivisionError:
    grps = 2

  if use_bottleneck or planes%2048 != 0:
    downsample = nn.Sequential(nn.Conv2d(planes, 2048, 1), nn.BatchNorm2d(2048))
    bottleneck = resnet_module.Bottleneck(planes, 512, groups = grps, downsample = downsample)
    return bottleneck
  return None

# Cell
class SSLModel(nn.Module):
  def __init__(self, encoder, global_rep = nn.Sequential(*[nn.AdaptiveAvgPool2d(1), Flatten()]),
               use_bottleneck = True, heads = [],
               global_head = None):
    super().__init__()
    encoder_layers = [encoder]

    planes = Encoding(encoder, size = (224, 224)).final_channel
    bottleneck = create_bottleneck(planes, use_bottleneck)
    if bottleneck is not None:
      encoder_layers.append(bottleneck)

    self.encoder = nn.Sequential(*encoder_layers)
    self.global_rep = global_rep

    if global_head:
      self.global_head = global_head

    if len(heads) > 0:
      self.heads = nn.ModuleList(heads)

  def forward(self, x, glob = True):
    x = self.encoder(x)
    out = []
    # Global Branch
    if glob:
      out.append(self.global_rep(x))
      if hasattr(self, 'global_head'):
        out[0] = self.global_head(out[0])

    # Other Branches
    if hasattr(self, 'heads'):
      features = []
      for head in self.heads:
        feat = head(x)
        features.append(feat)
      features = torch.cat(features, dim = 0)
      out.append(features)

    return out

# Cell
class ImageRepEncoder(nn.Module):
  def __init__(self, encoder, num_classes = None, use_bottleneck = True, custom_head = None):
    super().__init__()

    if not custom_head and not num_classes:
      raise ValueError('Provide number of clases needed for the classification head or provide a custom head')

    encoder_layers = [encoder]

    planes = Encoding(encoder, size = (224, 224)).final_channel
    bottleneck = create_bottleneck(planes, use_bottleneck)
    if bottleneck is not None:
      encoder_layers.append(bottleneck)

    self.encoder = nn.Sequential(*encoder_layers)
    if custom_head:
      self.head = custom_head
    else:
      self.head = nn.Sequential(nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(2048, num_classes))

  def forward(self, x):
    x = self.encoder(x)
    x = self.head(x)
    return x

# Cell
class SIMClrHead(nn.Module):
  def __init__(self, rep_dim):
    super().__init__()
    self.lin1 = nn.Linear(2048, 1024)
    self.relu = nn.ReLU()
    self.lin2 = nn.Linear(1024, rep_dim)
    self.norm = L2Norm()

  def forward(self, x):
    return self.norm(self.lin2(self.relu(self.lin1(x))))

# Cell
available_architectures = (arch_map.keys())
def cgd_encoder(arch:str, pretrained = True, **kwargs):
  if not isinstance(arch, str):
    raise ValueError('Pass the name of the architecture as the argument. eg resnet18')
  else:
    arch = arch.lower()

  if not arch in available_architectures:
    raise NotImplementedError(f'This architecture is not yet implemented for CGD model.\n The available architectures are {available_architectures}')

  return create_body(partial(rep_resnet, arch = arch, pretrained = pretrained), **kwargs)

# Cell
def SIMClrModel(encoder, rep_dim):
  return SSLModel(encoder, global_head = SIMClrHead(rep_dim),use_bottleneck = False)

# Cell
class SPoC(nn.Module):
  def forward(self, x):
    return torch.sum(x, dim = [-2, -1]).view(x.shape[0], x.shape[1], 1, 1)

# Cell
class GeM(nn.Module):
  def __init__(self, p = 1., learnable = False):
    super().__init__()
    if learnable:
      self.p = nn.Parameter(torch.tensor([p]))
    else:
      self.p = p

  def forward(self, x):
    return (x.pow(self.p).mean(dim = [-2, -1])).pow(1/self.p).view(x.shape[0], x.shape[1], 1, 1)

# Cell
cgd_config = {'S': SPoC, 'G': GeM, 'M': nn.AdaptiveMaxPool2d}
def cgd_descriptors(rep_dim, types = ['S', 'G', 'M'], general_max_pool_learnable = False,
                    general_max_pooling_p = 1., head_kwargs = {'use_conv_reduction':False, 'bdb':False}):
  """
  'S' -> for Sum Pooling Convolution,
  'G' -> for General Max Pooling,
  'M' -> Max Pooling.
  """
  descriptors = []
  for pool_type in types:
    if pool_type == 'G':
      pool = cgd_config[pool_type](general_max_pooling_p, general_max_pool_learnable)
    elif pool_type == 'M':
      pool = cgd_config[pool_type](1)
    else:
      pool = cgd_config[pool_type]()
    head = RepresentationHead(2048, rep_dim, pool = pool, **head_kwargs)
    descriptors.append(head)
  return descriptors